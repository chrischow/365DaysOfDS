---
layout: post
title: "Day 32: Python Keras"
subtitle: "Image Classification"
date: 2020-08-26 21:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today's work is a continuation of my CNN model for the **Multi-Type Aircraft Remote Sensing Images (MTARSI)** dataset.

Last night, I ran two more models, which had the same architectures as Model 1 and 2 respectively, except with an input shape of 70x70.

Accuracy for both models improved, and there was probably room for improvement:

#### Copy of Model 1:

<img src="/365DaysOfDS/img/posts/day032-01.png" style='margin-left: auto; margin-right: auto; display: block;'>

#### Copy of Model 2:

<img src="/365DaysOfDS/img/posts/day032-02.png" style='margin-left: auto; margin-right: auto; display: block;'>

## Another Mistake!
I found yet another mistake in my code. I was effectively training all my models using the base architecture (CCCNN-10) because I had an extra argument in my training function.

```py
def test_model(mdl_id, new_mdl_name, folder, conv_layers=[[32], [64], [64], [64]], top_layers=[[32]]):
    pass
```

## Lessons learnt
* **Check my code:** CNNs take a long time to train. Any fundamental errors will result in hard resets in the modelling process.
* **Start broad and zoom in later:** The testing process took so long because I imposed a strict requirement of 5 repeats of each architecture. A better approach would have been to do an extensive search of various architectures first, and then zoom in on the better models.

I'm starting over with the [panda](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc) approach:
* Run one iteration of each model, one at a time, with the following steps:
    * Monitor the results to roughly adjust the learning rate
    * Monitor the learning curve to adjust the patience setting
* Make adjustments to the model and start again with the following principles:
    * In the early stage, make bigger adjustments e.g. adding one layer to each block, doubling of neurons
    * In the later stage, make minor adjustments e.g. single layer to a block, increasing neurons in only a few layers

## The New Model

#### Iteration 1
Observations:
* Learning curve suggested that the model could be trained further

#### Iteration 2
Adjustments:
* Increased learning rate to test if model can be trained faster
* Increased patience setting to 50 to improve fit

Observations:
* Attained accuracy of 76%
* Learning curve: Unstable trajectory for validation loss and accuracy

#### Iteration 3
Adjustments:
* Decreased learning rate to 0.00001

Observations:
* Attained accuracy of 70%
* Early stopping at 483 epochs
* Training loss still decreasing: Model is underfit

#### Iteration 4
Adjustments:
* Doubled learning rate to 0.00002

Observations:
* Attained accuracy of 69%
* Training loss still decreasing, but validation loss flattened out and looks like it is on the verge of rising
* Still a large gap between training and validation loss

#### Iteration 5
Adjustments:
* Add back dropout for regularisation

Observations:
* Attained accuracy of 72%
* Reduced gap between training and validation loss

#### Iteration 6
Adjustments:
* Increase complexity: double the number of layers in each block
* Learning rate:
    * 0.00002 too slow
    * 0.00008 too fast

Observations:
* Training was really slow: I stopped it halfway to increase the learning rate

#### Iteration 7
