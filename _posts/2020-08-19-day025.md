---
layout: post
title: "Day 25: Python Keras"
subtitle: "Image Classification"
date: 2020-08-19 21:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today's work is a continuation of my CNN model for the **Multi-Type Aircraft of Remote Sensing Images (MTARSI)** dataset.

## Results from Model 1
I trained Model 1 overnight, and the results are as shown below. It looks like the model could do with a little more training.

<img src="/365DaysOfDS/img/posts/day025-01.png" style='margin-left: auto; margin-right: auto; display: block;'>

The accuracy also seems to have fallen with respect to the smaller MTARSI dataset (about 90% - see [Day 23](https://chrischow.github.io/365DaysOfDS/2020/08/17/day023.html)). This is probably because that dataset had only 12 classes, while this one has 22. But how does it fare with respect to some benchmarks?

### Logical Benchmarks
Given that we don't have standardised tests for aircraft recognition, we're going to use some logic to define some benchmarks. First, suppose we have someone who is completely untrained in aircraft recognition. He would probably use a simple heuristic to categorise the aircraft:

* Transport aircraft: Big, with widespread wings and look like they can carry lot's of things
* Light aircraft: Relatively small and harmless/clumsy-looking.
* Fighter aircraft: Relatively small, sharp, and fierce-looking.
* Other: Only the A-10 can't be fit into any of the above categories. It is a hybrid of light aircraft and transport aircraft.

Assuming he guesses the most prevalent class from each category (C-130 for transport aircraft, U-2 for light aircraft, F-15 for fighters, and the A-10 for others), he would get an accuracy of **20%**. In comparison, the model hit an accuracy of **65%**. We know for sure that the model beats the layman, but that doesn't mean a whole lot, given that using this model for operations would require a much higher accuracy.

On the other hand, someone trained in aircraft recognition would most likely be able to identify the aircraft close to **100%** of the time. The model doesn't look too good now.

### Performance vs. Efficiency
At the same time, this trained person would also have better things to do than sit at a computer, wait for a program to show him an image, and then label it with the correct aircraft type. I'm not sure it's justifiable to hire someone solely for this purpose for the long term. Therefore, there is some value in automating aircraft recognition, provided the accuracy is high enough. And that threshold depends on how critical it is to get the specific aircraft type right, and whether some other target (e.g. categories like transport/light/fighter/other) might be useful. 

## New Idea for Improving the Model: Progressive Resizing
I was irritated that CUDA crashed quite a few times as I was attempting to train the model. I did some Googling to resolve the issue and improve the efficiency of training my models and found some solutions. One easy fix was to downgrade Tensorflow back to V1. I'll try this later on. The solution that caught my eye (and that I was already thinking about) was to train the model using downsized images. There is a technique that involves doing exactly this, but building on it through transfer learning. It's called *progressive resizing*.

Essentially, in **[progressive resizing](https://towardsdatascience.com/boost-your-cnn-image-classifier-performance-with-progressive-resizing-in-keras-a7d96da06e20?gi=1361ee4339d9)**, you:

1. Train the model on downsized images, with a size of say 32x32. Save that model.
2. Build a new model that takes image inputs of a larger size, say 64x64. Then, append the old model in the subsequent layers, and train again. Save that model.
3. Build yet another model that takes image inputs of an even larger size, say 128x128. Do the same thing: append the model from step 2 in the subsequent layers, and train again.

This uses transfer learning to gradually build up a model. I'm guessing this is how researchers built huge models like [GoogLeNet](https://miro.medium.com/max/5176/1*ZFPOSAted10TPd3hBQU8iQ.png).

Applying the first step in this approach, I used an initial image size of 70x70, which is about one-third the median height/width of Model 1. Using my CPU, I was able to train Model 2 and Model 3 very quickly. Here are the results.

### Model 2
Model 2 differed from Model 1 in just two parameters: (1) a smaller input size of 70x70, and (2) a higher patience of 40 for early stopping. The training time was **a lot** lower, and the accuracy was about 1.6% higher.

<img src="/365DaysOfDS/img/posts/day025-02.png" style='margin-left: auto; margin-right: auto; display: block;'>

### Model 3
Model 3 differed from Model 2 in just one parameter: learning rate. Doubling the learning rate to 0.001 didn't help much. It took more epochs to converge, and resulted in a 1% drop in accuracy.

<img src="/365DaysOfDS/img/posts/day025-03.png" style='margin-left: auto; margin-right: auto; display: block;'>

### Evaluation
The fact that a much smaller 70x70 image was capable of producing slightly better results in a shorter time suggests that I could potentially reduce the image size further. Also, it could be that the architecture just sucks. Each of the 4 Conv blocks had only one convolutional layer and one max pooling layer. This probably resulted in information loss (from max pooling) in the earlier layers. Perhaps adding more convolutional layers before pooling could help.

Tomorrow, I'll test a smaller image size and vary the architecture.