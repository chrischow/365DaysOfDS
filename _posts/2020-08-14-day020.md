---
layout: post
title: "Day 20: Python Keras"
subtitle: "Neural Networks Basics"
date: 2020-08-14 23:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today, I continued on the **Complete Tensorflow 2 and Keras Deep Learning Bootcamp** on [Udemy](https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/). I completed a mini project on predicting whether borrowers would pay back their loan using a basic neural network.

The project used the famous LendingClub dataset, and involved the entire ML workflow:

* EDA
* Missing Data
* Feature Engineering
* Data Preprocessing
* Modelling

The full code from the course:

## Setup
```py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

```py
# Import data
df = pd.read_csv('../course_files/DATA/lending_club_loan_two.csv')
```

## EDA
```py
# Count plot of target
sns.countplot('loan_status', data=df)

# Loan Amount
sns.distplot(df.loan_amnt, kde=False); plt.show()

# Correlations
df.corr()

plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), cmap="GnBu"); plt.show()

# Installment vs. Loan Amount
print(df[['installment', 'loan_amnt']].corr())
sns.scatterplot(x='installment', y='loan_amnt', data=df); plt.show()

# Loan Status vs. Loan Amount
sns.boxplot(x='loan_status', y='loan_amnt', data=df)
df.groupby('loan_status')['loan_amnt'].describe()

# Grade and Sub Grade
sns.countplot('grade', data=df); plt.show()
sns.countplot('sub_grade', data=df); plt.show()

# Loan Status vs. Grade and Sub Grade
sns.countplot('grade', hue='loan_status', data=df)

sns.countplot('sub_grade',
              order=sorted(df.sub_grade.unique()),
              palette='RdBu_r',
              data=df); plt.show()

sns.countplot('sub_grade', hue='loan_status',
              order=sorted(df[df.grade.isin(['F', 'G'])].sub_grade.unique()),
              data=df[df.grade.isin(['F', 'G'])]
             )

# New target: Loan Repaid
# CODE HERE
df['loan_repaid'] = (df.loan_status == 'Fully Paid').astype(int)
df.loan_repaid.value_counts()

# Correlations with Loan Repaid
#CODE HERE
df[['int_rate', 'revol_util', 'dti', 'loan_amnt', 'installment', 'open_acc', 'pub_rec',
   'pub_rec_bankruptcies', 'revol_bal', 'total_acc', 'annual_inc', 'mort_acc',
   'loan_repaid']].corr().iloc[-1, :-1].plot.bar()
```

## Dealing with Missing Values

```py
# Percentage of missing values
df.isnull().mean()*100

# Drop Employment Title - too many categories
df = df.drop('emp_title', axis=1)

# Counts of Employment Length
sns.countplot('emp_length', order=df.emp_length.value_counts().index, data=df)

# Employment Length vs. Loan Status
sns.countplot('emp_length', hue='loan_status', data=df)

# Percentage of charged off
df['charged_off'] = 1-df.loan_repaid
df.groupby('emp_length').charged_off.mean().plot.bar()

# Drop Employment Length - no variation across categories
df = df.drop('emp_length', axis=1)

# Drop Title - repetition
df = df.drop('title', axis=1)

# Fill missing values in mort_acc using average mort_acc for each total_acc (most correlated feature)
total_acc_avg = df.groupby('total_acc').mort_acc.mean()

def fill_mort_acc(total_acc, mort_acc):
    if np.isnan(mort_acc):
        return total_acc_avg[total_acc]
    else:
        return mort_acc
    
df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)

# Remove remaining rows with missing data
df = df.dropna()
```

## Feature Engineering
```py
# SOMETHING NEW TO ME: Select columns by type
df.select_dtypes('object').columns

# Convert term from string to numeric
df['term'] = df.term.str.replace(' months', '').astype(int)
df.term.value_counts()

# Drop Grade - subset of Sub Grade
df = df.drop('grade', axis=1)

# Create dummies
df = pd.get_dummies(df, columns=['sub_grade'], drop_first=True)
df = pd.get_dummies(df, columns=['verification_status', 'application_type','initial_list_status','purpose'], drop_first=True)

# Re-categorise home_ownership
df['home_ownership'] = df.home_ownership.replace(['NONE', 'ANY'], 'OTHER')
df = pd.get_dummies(df, columns=['home_ownership'], drop_first=True)

# Extract zip code and create dummies
df['zip_code'] = df.address.str[-5:]
df = pd.get_dummies(df, columns=['zip_code'])
df = df.drop('address', axis=1)

# Remove Issue Date - leakage
df = df.drop('issue_d', axis=1)

# Extract credit line year
df['earliest_cr_year'] = df.earliest_cr_line.str.replace('.*-', '').astype(int)
df = df.drop('earliest_cr_line', axis=1)
```

## "Preprocessing"
I would actually include this as part of modelling because I think of models as a pipeline which **includes preprocessing steps**. However, I suppose that in this example where we create only one fit, it is kind of separate from modelling.

```py
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Sample data to avoid memory issues
df = df.sample(frac=0.3,random_state=101)

# Split data
X = df.drop('loan_repaid', axis=1).values
y = df.loan_repaid.values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

# Scale data
mms = MinMaxScaler()
X_train = mms.fit_transform(X_train)
X_test = mms.transform(X_test)
```

## Modelling

```py
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Set up model
model = Sequential()

# Add layers based on recommended number of layers and nodes
model.add(Dense(78, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(39, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(19, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

# Compile
model.compile(
    optimizer='adam',
    loss='binary_crossentropy'
)

# Fit
model.fit(X_train, y_train, epochs=25, batch_size=256, validation_data=(X_test, y_test))
```

## Evaluate Model

```py
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Plot learning curve
losses = pd.DataFrame(model.history.history)
losses.plot()

# Make predictions
predictions = model.predict_classes(X_test)

# Print metrics
print(classification_report(y_test, predictions))
print(roc_auc_score(y_test, predictions))
```

I did not code along with the Tensorboard section due to some errors on my PC. Apparently, the filenames for the log files were too long. Will come back to this again some time.