---
layout: post
title: "Day 16: PySpark"
subtitle: "Cleaning Data with PySpark"
date: 2020-08-10 19:00:00 +0800
background: '/img/posts/computing.jpg'
---

Building on the (re-)introduction to PySpark yesterday, I completed the **Cleaning Data with PySpark** course on [DataCamp](https://learn.datacamp.com/courses/cleaning-data-with-pyspark). While PySpark is likely the clear winner when it comes to performance in processing large datasets, its functions are much more limited compared to Pandas. I think it'll come in handy when I work with image data for competitions on Kaggle - this is my next focus area to get acquainted with deep learning.

## Working with DataFrames (continued)

### Dropping Columns
```py
df.drop('Col1')
```

### String Operations

```py
import pyspark.sql.functions as F

# Upper case
df.withColumn('upper', F.upper('name'))

# Split
df.withColumn('splits', F.split('name', ' '))

# Case
df.withColumn('year', df['year_string'].cast(IntegerType()))
```

### `ArrayType()` Operations
* Analgous to lists in Python

```py
# Length of arrayType() column
F.size('ArrayTypeColName')

# Retrieve an item at a specific index
df.Column1.getItem(0)
```

### Conditional Column Operations
Use `.when` and `.otherwise` from `pyspark.sql.functions`.

#### `.when`
```py
# Returns a dataframe with the full Name and Age columns, and a new unnamed column that has Adult for entries where Age >= 18, and nulls otherwise
df.select(df.Name, df.Age, F.when(df.Age >= 18, "Adult"))

# Chain multiple when statements
df.select(df.Name, df.Age, F.when(df.Age >= 18, "Adult").when(df.Age < 18, "Minor"))

# Add otherwise - can only have one per change
df.select(df.Name, df.Age, F.when(df.Age >= 18, "Adult").otherwise("Minor"))
```

### User-defined Functions
* Python methods wrapped via `pyspark.sql.functions.udf`
* Stored as variable and can be called as a normal Spark function

```py
# Define new function
def reverseString(mystr):
    return mystr[::-1]

# Wrap and store
udfReverseString = F.udf(ReverseString, StringType())

# Use with Spark
user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))
```

### IDs in PySpark
* Since no transformations are done until an action is performed, steps may be rearranged out of the order specified e.g. the assignment of IDs
* Normal increasing, sequential ID fields are not parallel
* Spark uses monotonically increasing IDs:
    * `pyspark.sql.functions.monotonically_increasing_id()`
    * Increase in value
    * Not necessarily sequential
    * Completely parallel

## Performance Optimisation

### Caching

#### Benefits
* Stores dataframes in memory or on disk
* Improves speed
* Reduces resource usage

#### Disadvantages
* May not fit in memory
* Lifetime of cached objects may not be available

#### Using Caching
* Guidelines:
    * Cache only if required
    * Test caching at different points of the pipeline to see if performance improves
    * Cache in memory and fast SSD storage
    * Use intermediate Parquet files as checkpoints in case jobs fail mid-task
    * Stop caching objects when finished to free up resources for other dataframes
* Is one transformation that is not performed until action is requested

```py
# Cache data
df = df.cache()

# Check if dataframe is cached
df.is_cached

# Stop caching once done
df.unpersist()
```

### Import
* Spark clusters are made of **driver** and **worker** processes
* Parameters:
    * Number of objects (e.g. files, network locations): More objects with approximately equal quantity of rows are better than fewer larger objects
    * General size of objects
    * Well-defined schemas: avoids reading the data multiple times, and provide validation on import
* On split files:
    * Can be read in as ```spark.read.csv('filename_*.csv')

### Cluster Configurations

#### Read and Write Configurations

```py
# Read
spark.conf.get(ConfigurationName)

# Write
spark.conf.set(ConfigurationName)
```

#### Cluster Types
Deployment options:
* Single node: All components on a single system e.g. physical, VM or container
* Standalone: Dedicated machines as driver and workers
* Managed: Components are handled by third-party cluster manager e.g. Kubernetes

Driver:
* Assigning and managing tasks
* Consolidating results
* Shared data access
* Tips:
    * Should have double the memory of worker nodes
    * Use fast local storage

Worker:
* Runs actual tasks
* Has copy of all code, data, and resources for a given task
* Tips:
    * More is better than larger ones
    * Need to test to find balance
    * Should use fast local storage

### Shuffling
* To check how Spark plans to execute a task, use `.explain`

```py
# Example: checking distinct
df2 = df.select(df['Column1']).distinct()

# Check steps - returns the plan
df2.explain()
```
* Shuffling is used to move data around to various workers to complete a task
* Lowers throughput, so should limit this
* Limiting shuffling
    * Limit use of `.repartition(num_partitions)` - use `.coalesce(num_partitions)` instead
    * Use care when calling `.join()` - use `.broadcast()`

### Broadcasting
* Provides a copy of the object to each worker
* Prevents excess communication betweten nodes
* Can drastically speed up `.join` operations
* Will slow performance when used (1) with very small dataframes or (2) broadcasting the larger dataframe in a join
* Using broadcast:

```py
from pyspark.sql.functions import broadcast

# Join with broadcast
combined_df = df_1.join(broadcast(df_2))
```

## Data Pipelines
* Set of steps to process data from source to final output
* Can span many systems
* Comprises inputs, transformations, and outputs
    * Inputs: CSV, JSON, web services, databases
    * Transformations: Adding columns, performing calculations, filtering, dropping
    * Outputs: CSV, Parquet, database
    * Validation: Testing data to verify it is as expected

### Parsing Data
* Common issues with CSV data
    * Incorrect data: Empty rows, commented lines, headers
    * Nested structures: Multiple delimiters
    * Non-regular data: Differing number of columns per row
* Spark's CSV parser:
    * Automatically removes blank lines
    * Removes comments using optional argument: `comment=str` argument
    * Handles header fields: `header=Bool` argument
    * Automatic column creation `sep=str` argument, defaulting to `,`

### Data Validation
* Verifying that data complies with expected format
    * Rows and columns
    * Data types
    * Other validation rules e.g. physically possible quantities
* Validating via joins:
    * Compares data against known values
    * Easy to find data in a given set
    * Comparatively fast
* Complex rule validation
    * Verifying against external source
    * Likely uses UDF