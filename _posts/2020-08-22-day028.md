---
layout: post
title: "Day 28: Python Keras"
subtitle: "Image Classification"
date: 2020-08-22 20:30:00 +0800
background: '/img/posts/ml.jpg'
---

Today's work is a continuation of my CNN model for the **Multi-Type Aircraft of Remote Sensing Images (MTARSI)** dataset.

## Results from 8 Variants of CCCNN-10
I ran 8 new models overnight. See the table below for the variations and the results:

<img src="/365DaysOfDS/img/posts/day028-01.png" style='margin-left: auto; margin-right: auto; display: block;'>

The key findings were:

* **In the top block:** Adding nodes to the FC layer was somewhat useful, but adding one additional FC layer with the same number of nodes was better
* This time, adding Conv3 layers to the earlier blocks seemed to have a better effect on both loss and accuracy than adding them in the later blocks


## X New Variants

### CCCNN-19
This variant of CCCNN-10 tests the **patience** setting for early stopping. I realised that I didn't pay much attention to the learning curves, and inspected them properly today. None of the curves followed the classic learning curve shape. Hence, I decided to increase the patience setting to see whether we are underfitting. If we are, validation loss should decrease further. If we are not, the learning curves should more closely resemble the classic curve.

Increasing the patience setting from 40 to 80 resulted in the discovery of a new low for validation loss. At epoch 108, loss reached a low of 1.3855, which was the lowest point until 57 epochs later, where it dropped to 1.3824. 
This suggests that a patience of 40 epochs might be stopping the training a little too early.

### CCCNN-20
We'll do one more experiment to verify these findings: CCCNN-17 with a patience of 80. We found the following:

* Accuracy was close
* Loss was about 10% higher

These experiments showed that there was considerable variability in model performance. These models both had big differences in loss, even though they had the same architecture. Hence, we need a better estimate of the "typical" accuracy and loss through repeated runs.

### CCCNN-21
Just for fun, I increased the number of nodes in the Conv layer in block 4 from 64 to 128. This didn't produce significantly better results. But, as I noted above, one run is not sufficient to test out a configuration.

## Reflections

### Randomness
This is a big learning pointer. I completely underestimated how random the results from NNs could be. Typically, for traditional ML algorithms, I would use **repeated cross validation** or **Monte Carlo cross validation** to test my models. With blazing fast algorithms like LightGBM, I could get my results back in about 10-15 minutes, depending on the size of the dataset. Yet, I didn't hold my NNs to that standard of testing, partially because it takes so damn long to train a model (in relative terms).

The standards for the estimation of model performance metrics have not changed. You still need a large enough sample of performance scores to report the expected performance of a model, regardless of how long it takes to train. Hence, I will do repeated runs for selected models to check the distribution of validation loss and accuracy.

First, I've selected CCCNN-10 because that's where the first big jump up in accuracy occurred - from increasing the number of nodes in the FC layer. It's unlikely that the variance in performance is so large that accuracy would drop to 60+%, and that the loss would rise to 1.70+ (in CCCNN-1 to CCCNN-9). Hence, this architecture is worth testing further.

Second, I've also selected CCCNN-17 for further testing because (1) its architecture is slightly more complex than CCCNN-10 and (2) it clocked the lowest loss thus far, even with the old patience setting of 40 epochs.

Shall do these tomorrow.

### Minibatch Training for the Entire Modelling Process
I've learned that the modelling process is similar to the way NNs are trained. Using minibatch training is advantageous because it gives the model more frequent feedback within each pass through the data, as opposed to a one-time update after each epoch. Similarly, the modelling process I employed uses a relatively large "batch size". From the best model so far, I create numerous variants, run them overnight, and evaluate them the next day. And I have to do this because I have a day job. The modelling process was a lot more adaptive today because I could make adjustments to the configurations I wanted to test based on the last tested model. That is, my batch size is effectively 1.