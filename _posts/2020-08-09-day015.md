---
layout: post
title: "Day 15: PySpark"
subtitle: "Basics of PySpark"
date: 2020-08-09 22:30:00 +0800
background: '/img/posts/computing.jpg'
---

My focus today was on PySpark. A production ML system needs to be scalable and capable of dealing with large datasets. PySpark is an important technology because it meets these criteria.

I re-did two sections of the **Introduction to PySpark** course on [DataCamp](https://learn.datacamp.com/courses/introduction-to-pyspark). Today's session was very light:

* Creating sessions
* Running queries
* Basic operations on DataFrame columns

I'll dive deeper into a more in-depth course on PySpark.

```py
# Import SparkSession
from pyspark.sql import SparkSession

# Create session
spark = SparkSession.builder.getOrCreate()

# List tables in catalog
spark.catalog.listTables()

# Setup up query
query = "SELECT * FROM flights LIMIT 10"

# Run query
flights10 = spark.sql(query)

# Convert to pandas
pd_flights10 = flights10.toPandas()

# Create spark dataframe from pandas datafram df
spark_temp = spark.createDataFrame(df)

# Create temporary view
spark_temp.createOrReplaceTempView('temp')

# Read from CSV
data = spark.read.csv(file_path, header=True)

# Pull table from Spark - creates a local copy
df = spark.table('df')

# Create a new column
df = df.withColumn('new_column', flights.OldColumn + 1)

# Equivalent queries for filtering rows
df.filter("Column1 > 50").show()
df.filter(df.Column1 > 50)

# Equivalent queries for selecting columns
df.select('Column1', 'Column2')
df.select(df.Column1, df.Column2)

# Creating new column
newCol = df.Column1/60

# Creating new column + renaming
newCol = (df.Column1/60).alias('newcolumn')

# Selection + creation
df_subset = df.select('Column1', 'Column2', 'Column3', newCol)
df_subset = df.selectExpr('Column1', 'Column2', 'Column3', 'Column1/60 as newcolumn')

# Rename column
df = df.withColumnRenamed('ColumnToRename', 'NewColumnName')

# Normal aggregation functions e.g. min, max, count, avg
df.groupBy().min('Column1').show()

# Aggregation with SQL functions
import pyspark.sql.functions as F

by_col1_col2 = df.groupBy('Column1', 'Column2')
by_col1_col2.agg(F.stddev('Column3')).show()
```