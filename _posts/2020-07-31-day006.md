---
layout: post
title: "Day 6: Python"
subtitle: "Automated EDA: Detecting Data Formats"
date: 2020-07-31 20:00:00 +0800
background: '/img/posts/ml.jpg'
---

## Automated Machine Learning
This week, I attended a 4-day ~~marketing pitch~~ course by DataRobot, the company behind its eponymous automated machine learning platform. I was incredibly impressed by the features that the platform offers. DataRobot has gone to great lengths to provide users with a great-looking GUI that abstracts all the tedious stuff that data scientists would otherwise have to do by hand. But, I have some concerns about automating machine laerning, and will write more about this on [Data&Stuff](https://chrischow.github.io/dataandstuff/).

One of the value propositions of DataRobot is that it saves data scientists the effort of digging through their old code, copying what's relevant, and making minor modifications to re-purpose it to the new project at hand. This made me realise that in the past few years, I actually created some of DataRobot's features in my past projects. And that means it's possible to *create my own app* to consolidate my code! This is the subject of today's post. I'll start by looking into automating the mechanical aspects of Exploratory Data Analysis (EDA).

## Automating EDA
What would an ideal EDA tool look like? I think a good tool would:

1. **Provide a good overview of the dataset:** This would include things like the size of the dataset, recommend data types, analyse missing values, allow us to very quickly inspect the dataset, and provide summary statistics. ***The overview should give us broad intuition about what's in the data.***
2. **Provide detailed info about individual features:** The feature set should be broken down into *numeric* and *categorical* feature groups. Then, for each group,the tool should provide univariate statistics accompanied with graphs to summarise the data. ***This function should give us ideas for (a) data cleaning and (b) feature engineering.***
3. **Provide a function to explore feature interactions:** There should be functions to explore the interaction between numeric and categorical features, and provide measures of statistical significance. ***This function should give us ideas for the extraction of interaction features.***
4. **Provide a function to explore feature importance:** Importance is with respect to the target feature. Hence, the tool should make us choose a target, and compute statistical measures to estimate if each feature (and possibly groups of features) has some relation to the target. This would inform feature selection later on. ***This function should give us ideas on (a) features to select, (b) the type of algorithm (e.g. linear algos for linear relationships, or trees for more complex, non-linear relationships), and (c) the need for feature engineering (if few of the features have any strong relation to the target in their current form).***

Ultimately, my idea of a useful automated EDA tool is one that provides ***good intuition*** and ***good recommendations***. It is **NOT** meant to automatically process the data for you. The implicit philosophy here is that **domain knowledge must be carefully interlaced with data science techniques**. This ensures that the process (e.g. hypotheses, data manipulation, model development) is robust. The only way to achieve this is to ensure minimum separation between domain expertise and data science expertise. The ideal case would be for people with both sets of expertise to process the data. The next thing we can do is to ensure maximum interaction between the domain experts and the data scientists. This is all a little philosophical, so I'll save this for the blog post on automated machine learning. Let's get on with the coding.

## Data Formats
The very first thing we need to do when we ingest data is to ensure that our features are in the right format. Drawing from the PostgreSQL course, the possible data types that we might want to identify are:

* Boolean
* Numeric:
    * Integer
    * Float
* Date and Time:
    * Date
    * Time
    * Datetime
* String

The list above is ordered to represent our strategy. First, we separate booleans from non-booleans, since these are the easiest to detect. Second, we differentiate between numerics and non-numerics (the rest). Then, we work our way down the list to identify the other data types, which require more complex rules.

I am going to be extremely lazy and build the tool on top of Pandas, because Pandas is already amazing at detecting data types. 

### Data Preparation
Before we dive into detecting numerics, we need to ensure the data is of a clean format so that we can make use of Pandas' functions. The function below does the following:

1. Convert the data series into a `pandas.Series` object.
2. Clean the string by stripping whitespace and converting it into lower case.
3. Replace some common missing values with NaNs (missing values).
4. Remove all missing values.
5. Compute the total number of valid values. This will be used to propose a 'categorical feature' tag later on.

Note that we're not doing a full cleaning of the original features just yet. We're only doing just enough cleaning to identify data types.

```py
# Extract valid entries
def get_valid(x):
    
    # Convert to series
    xclean = pd.Series(x, dtype=str)
    
    # Strip and lower
    xclean = xclean.str.strip().str.lower()
    
    # Convert missing values to NAs
    missing_vals = ['', 'null', 'none', '#n/a', 'nan']
    for val in missing_vals:
        xclean[xclean == val] = np.nan
    
    # Get valid entries
    xclean = xclean[xclean.notnull()]
    
    return xclean.shape[0], xclean
```

### Booleans
Booleans are the easiest to detect. The heuristic we will use is: the feature has only 1 or 2 unique values. This is because the content and length of the strings in the feature do not matter. This captures features that have True/False, Yes/No, Y/N, A/B, Apple/Durian.

```py
# Detect booleans
def detect_boolean(x):
    # Get valid entries
    total_valid, xvalid = get_valid(x)
    
    if xvalid.nunique() <= 2:
        return 'bool'
    else:
        return 'non-numeric'
```

### Numerics
Next, we want to detect numerics because they have relatively strict formats, and therefore are easy to detect. If Pandas is able to convert a feature to a numeric type, we can be sure that it is indeed numeric.

#### Integers
If successfully converted, integer features will be reflected as `intX` where `X` is 8, 16, 32, or 64. Hence, we only need to search for the string "int" in the converted feature's data type.

A special case of "false integers" is where the string starts with a zero, followed by another digit e.g. `0131`. This is **not** an integer: it is probably best to keep this as a string.

#### Floats
If successfully converted, float features will be reflected as `floatX` where `X` is 32 or 64. Hence, we only need to search for the string "float" in the converted feature's data type.

```py
# Detect numeric
def detect_numeric(x):
    
    # Get valid entries
    total_valid, xvalid = get_valid(x)
    
    try:
        # Try converting to numeric
        xconv = pd.to_numeric(xvalid)

    except ValueError:
        return 'non-numeric'
    
    if xvalid.str.contains('^0[0-9]').sum() > 0:
        return 'string'
    elif 'int' in str(xconv.dtypes):
        return 'integer'
    elif 'float' in str(xconv.dtypes)::
        return 'float'
```

### Dates and Times
We're going to be real lazy again. We're going to make use of Pandas' `pd_to_datetime` function to automatically parse dates. If the raw dates can't be parsed, it probably means that they aren't valid dates to begin with.

```py
# Detect datetime
def detect_datetime(x):
    
    # Get valid entries
    total_valid, xvalid = get_valid(x)
    
    try:
        # Try converting to datetime
        xconv = pd.to_datetime(xvalid)
        
    except ValueError:
        
        return 'string'
    
    if 'datetime' in str(xconv.dtypes):
        return 'datetime'
    else:
        return 'string'
```

### The Full Code
Now, we can combine all the detectors above using the following logic:

* If it has two or fewer unique values: Tag as boolean
* Else if it can be converted to numeric:
    * If features contains values that start with `00`: Tag as string
    * Else if data type is integer: Tag as integer
    * Else if data type is float: Tag as float
* Else if it can be converted to datetime: Tag as Date time
* Else: Tag as string

```py
# Detect data types
def detect_dtype(x):
    
    # Get valid entries
    total_valid, xvalid = get_valid(x)
    
    # Check for boolean first
    if xvalid.nunique() <= 2:
        return 'bool'
    
    # Check for numeric
    try:
        xconv = pd.to_numeric(xvalid)

    except ValueError:
        
        # Check for datetime
        try:
            # Try converting to datetime
            xconv = pd.to_datetime(xvalid)

        except ValueError:
            return 'string'
        
        if 'datetime' in str(xconv.dtypes):
            return 'datetime'
    
    if xvalid.str.contains('^00').sum() > 0:
        return 'string'
    if 'int' in str(xconv.dtypes):
        return 'integer'
    if 'float' in str(xconv.dtypes):
        return 'float'
    
    # Catchall: string
    return 'string'
```

That gives us our data detector function! Next, we will need to develop a function to detect other characteristics about the features.