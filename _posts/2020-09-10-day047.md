---
layout: post
title: "Day 47: Data Science Readings"
subtitle: "Machine Learning"
date: 2020-09-10 22:00:00 +0800
background: '/img/posts/ml.jpg'
---

I'm continuing with my summary on [Machine Learning Yearning by Andrew Ng](https://www.goodreads.com/book/show/30741739-machine-learning-yearning).

## Bias and Variance

### Introduction
* Bias: Error rate
* Variance: How much worse the algorithm does on the dev (or test) set compared to the training set

### Scenarios

| Bias | Variance | Description  |
| :--: | :------: | :----------- |
| Low  | High     | Overfitting  |
| High | Low      | Underfitting |
| High | High     | Unclear      |
| Low  | Low      | Ideal Situation  |

### Comparing to Optimal Error Rate
* Estimate the optimal error rate (the best you can do) for a problem
    * E.g. Recognising cats = 0% for humans
    * E.g. Recognising speech in noisy environments = Maybe 10% for humans
* Also called Bayes error rate or Bayes rate in statistic
* Bias and variance should be compared to this

### Addressing Bias and Variance
* High bias:
    * Increase the size of your model (e.g. add layers/neurons)
    * Might increase variance and risk of overfitting if regularisation is not being used
    * With regularisation or dropout, performance will stay the same or improve
* High variance: Add data to training set
* Innovative model architectures:
    * Use papers on NNs to get ideas
    * Results are less predictable

### Bias-Variance Tradeoff
* Changes that reduce bias at the cost of variance:
    * Increasing size of model
* Changes that reduce variance but increase bias:
    * Regularisation
* Changes that improve both:
    * Adding input features
    * Proper selection of model architecture (but it's difficult)
* Changes that improve variance:
    * Add training data

### Techniques for Reducing Bias
* Increase the model size e.g. neurons/layers
* Modify input features based on insights from error analysis
    * Perform error analysis on **training data**
    * Select random number of misclassified examples and work through the steps from the Error Analysis chapter
* Reduce or eliminate regularisation
* Modify model architecture

### Techniques for Reducing Variance
* Add more training data
* Add regularisation (also increases bias)
* Add early stopping (based on dev set error)
* Feature selection to decrease number/type of input features
    * Might also increase bias
    * Large reductions would have a bigger effect
    * Less important for deep learning when data is plentiful, more important for smaller datasets
* Decrease model size
    * Could increase bias
    * Not recommended unless you want to reduce computational cost
* Modify input features based on insights from error analysis (also reduces bias)
* Modify model architecture (also reduces bias)

## Learning Curves

### Diagnosing Bias and Variance
* The learning curve:
    * Dev set error:
        * Plots dev set error against number of training examples
        * Run model on different training set sizes
        * As the training set size increases, the dev set error should decrease
        * Add a line for the desired performance: Based on human-level performance and business sense
    * Training error:
        * Training error usually increases as the training set grows: the model finds it harder to memorise the training examples when there is more data
        * Gives confidence to extrapolate the dev set error curve
* Extrapolate from the dev set curve to figure out whether the desired performance can be reached by adding data

High Bias:
* Large gap between training error and desired performance
* Gap between training and dev curves is small, indicating low variance
* Cannot be certain when:
    * Dev is small

High Variance:
* Training error is relatively low
* Dev set error is much higher
* Depending on the slope of the curves, adding data could help

### Learning Curves vs. Dataset Size
* For very small datasets, be wary of:
    * "Bad" or "good" training sets
    * Class imbalance, which leads to unrepresentative training sets
* Solutions to use after you have discerned that the curves are too noisy
    * For the learning curve, create several randomly chosen training sets comprising a low number of examples e.g. 10 of 100 **with replacement**, traing the model, and compute the average training and dev set error
    * Use stratification
* For large datasets, increase the interval for training set sizes to reduce computational cost

## Comparing to Human-Level Performance
* Building an ML system is easier if people can do it well because:
    1. Ease of obtaining data from human labellers: people can provide accurate labels
    2. Error analysis can draw on human intuition:
    3. Use human-level performance to estimate the optimal error rate and set a desired error rate
* Use the highest level of performance that can be feasibly be obtained from a human equivalent
* In error analysis, understanding human-level performance is still important for examples that machines typically get right but the machine didn't

## Training and Testing on Different Distributions

### When to Do It
* All the time if possible
* If true-distribution data is limited, put half into dev/test, and the other half in training

### How to Decide Whether to Use All Your Data
* Traditional learning algorithms: Mixing different distributions would have a risk of worse performance
* Deep learning: Risk is greatly diminished
* Effects of different distributions:
    * It gives the model more examples of what the correct labels are
    * Model expends more capacity learning about properties from the incorrect distribution
* If you have enough computational capacity to build a big enough neural network, having different distributions is not a problem
* Data that is from a completely different distribution that adds no value (e.g. historical documents to recognise cats?!), leave it out

### Weighting Data
* If there are 10x incorrect-distribution examples as there are correct-distribution examples, you should need 10x more computational resources to model both groups of examples
* Weighting examples helps to ensure the model does well on both distributions by assigning a 1/10 weight on the loss from the incorrect-distribution examples
* Impact: don't need to build such a big NN

### Generalising from the Training Set to the Dev Set
* Data mismatch:
    * Model generalises well to new data drawn from the same distribution as the training set
    * If model does well on training dev set (below) but not on the dev set, there is data mismatch
* Split the training set into a **training dev set** which has the same distribution as the training set
* Dev/test sets will continue to reflect the true distribution

### Addressing Data Mismatch
* Try to understand what properties of the data differ between the training and the dev set distributions
* Try to find more training data that better matches the dev set examples

### Artificial Data Synthesis
* Examples:
    * Add noise to speech for speech recognition
    * Add motion blur to images for image recognition
* Beware that synthetic data would appear realistic to a human, but not necessarily to a human e.g. same noise added to speech will cause model to overfit to that noise clip
* Think whether the synthesised data properties are representative of true distribution

## Debugging Inference Algorithms

### The Optimisation Verification Test
*(Doesn't sound very related to simpler ML problems)*

When the model makes incorrect predictions, there are two possibilities for what went wrong:
* Search algorithm problem: the approximate search algorithm failed to find the value of the proposed output that maximises the score of that output
* Objective (scoring function) problem: The function that calculates the score is inaccurate

To diagnose what's wrong, run the Optimisation Verification test by comparing the score of the optimal output against the score of the prediction:
* If the score of the optimal output is higher, it means the search/optimisation algorithm is failing
* If the score of the optimal output is lower, the scoring function is wrong

Thoughts:
* For classification, the correct class will always have a loss of zero
* Therefore, the scoring function will never be wrong, and the fault lies entirely with the optimisation algorithm
* Where does the OV test apply?
    * Problems with more complex scoring functions like speech recognition and translation
    * Custom scoring functions: reinforcement learning

## End-to-End Deep Learning

### Introduction to End-to-End Algorithms
* Learning algorithm maps direct inputs into a desired output
* Intermediate steps (e.g. parsing) are no longer done - the single end-to-end algorithm takes care of everything
* Examples:
    * Sentiment prediction
    * Autonomous driving
    * Speech to text
* Pros:
   * Large dataset: works very well
   * Can learn rich output e.g. text, transcriptions, audio
* Cons:
    * When we don't have much data, hand-engineering features based on knowledge may be more useful
    * Cannot conduct error analysis by parts

### Pipelines
1. Consider how easy it is to collect training data
    * If easy to obtain component data or hard to obtain joint whole-system data: use components
2. Consider how simple the tasks are to be solved by individual components
    * Tasks that are easier to learn: use components
    * Build components that handle simple tasks as opposed to an end-to-end system

## Error Analysis By Parts
* To figure out which parts of the pipeline are problematic
* If there is a human benchmark, check the performance relative to human-level performance for each component and the system as a whole
* Eliminate from error analysis components that are already close
* If errors are ambiguous, manually correct the earlier inputs and test if the components downstream are causing errors
* If all components in the pipeline are performing close to human level, the pipeline is flawed: expand or reconfigure pipeline