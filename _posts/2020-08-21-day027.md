---
layout: post
title: "Day 27: Python Keras"
subtitle: "Image Classification"
date: 2020-08-21 22:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today's work is a continuation of my CNN model for the **Multi-Type Aircraft Remote Sensing Images (MTARSI)** dataset.

## New Testing Approach

### Using Scripts to Run Models Sequentially
Last night, I wrote two scripts to optimise my work process. First, `cccnn.py` (Chris Chow's Convolutional Neural Networks) contains functions to (1) build a model, (2) fit it, and (3) export the results. Second, `modelx.py` contains the model configurations I want to test using the functions from `cccnn.py`. I only need to run `modelx.py` in Bash to sequentially train multiple models.

### Learning from the Experts
After studying [VGG's experimentation approach](https://becominghuman.ai/what-is-the-vgg-neural-network-a590caa72643?gi=5c97bf8d6561) further, I noticed a few patterns:

* They made very few changes from model to model (as mentioned in yesterday's post)
* VGGNet reduced images into a lot of relatively small feature maps of 7x7
* VGGNet started with fewer Conv layers with fewer filters in the earlier blocks to more Conv layers with much more filters in the later blocks
* VGGNet used a large number of nodes (and three layers of these!) in the top layer, prior to the output layer

## Results from 5 Variants of CCCNN-10
Considering this approach, I started again from CCCNN-2 (new name for Model 2), creating 5 new models, CCCNN-6 to CCCNN-10, each of which had exactly one parameter changed from the CCCNN-2 architecture. CCCNN-6 to CCCNN-9 had changes in the number of Conv layers within the blocks, while CCCNN-10 simply had more nodes in the fully-connected (FC) layer. See the table below:

<img src="/365DaysOfDS/img/posts/day027-01.png" style='margin-left: auto; margin-right: auto; display: block;'>

From the results (see validation accuracy above), here are the findings:

1. Adding Conv layers to the earlier blocks didn't help much
2. Adding Conv layers to the later blocks helped a little
3. Increasing the number of nodes in the FC layer **greatly** improved accuracy

### Nodes in FC Layer
For point 3, it appears that 32 nodes in the FC layer was too little. Perhaps, running 2,304 outputs from the Flatten layer into 32 nodes may have caused the model to underfit. Next, I ought to try including more nodes, and including more FC layers in the top block.

### Key Metric
Currently, I'm checkpointing models based on their validation loss, and looking primarily at their accuracy for selecting the new base model to build from. The intuition is that loss is a better measure of the model's predictions. While accuracy is much more intuitive, it doesn't include any measure of confidence. For example, a model that predicts class A with a probability of 51% and B with a probability of 49% would output A as the prediction. There's information loss when you convert a probability to a discrete output.

Loss, on the other hand, accounts for the probability score output by the model. I find that a little more useful than a binary indicator because you can do more things with probabilities. For example, if the highest predicted probability is below a certain threshold, you could output the next most probable class to allow the operator to make a better decision. Or, you could simply output as many classes required to ensure their predicted probabilities sum up to at least 90% or 95%.

## Connecting the GPU
I finally managed to connect my GPU! The problem for me was that training would freeze at the last batch of a random epoch. I would need to restart Jupyter and sometimes my computer. Thereafter, running the code would result in another freeze.

The solution that worked for me was:

1. Update NVIDIA graphics driver
2. Install CUDA toolkit
3. Install CuDNN

My earlier attempts to resolve the issue failed because I did step 2 and 3 without doing step 1. This procedure worked, and my code runs 2.5x as fast now!

## Next Iteration
I've got 8 models lined up. These are variants of CCCNN-10 with additional Conv layers in the various blocks. Will run this overnight and check the results tomorrow.