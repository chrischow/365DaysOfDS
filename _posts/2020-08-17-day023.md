---
layout: post
title: "Day 23: Python Keras"
subtitle: "Image Classification"
date: 2020-08-17 23:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today, I continued work on my image classification model on the **Multi-Type Aircraft of Remote Sensing Images (MTARSI)** dataset. I did some trial and error, and learned a few things about neural networks.

### GPU Utilisation
When training my model, I realised that my GPU memory was fully utilised, but its utilisation was extremely low. Apparently, this was because the dataset was small, and did not require much processing.

### Validation Loss vs. Training Loss
Yesterday, I detected an issue where the training loss was way higher than the validation loss. As such, I suspected that the validation samples I chose were "too easy". I tried to remedy this by using the `ImageDataGenerator`'s `validation_split` parameter to let Keras split my samples for me. In addition, this would augment the validation data with horizontal/vertical reflections, zooming, shearing and other transformations I applied to the training data. I thought that this would make the validation samples more "difficult". However, there was no change to the result. Here's the loss:

<img src="/365DaysOfDS/img/posts/day023-01.png" style='margin-left: auto; margin-right: auto; display: block;'>

Here's the accuracy:

<img src="/365DaysOfDS/img/posts/day023-02.png" style='margin-left: auto; margin-right: auto; display: block;'>

This is probably because the images in the entire dataset were generated from the same few images. And that means that this version of the MTARSI dataset (only 2,200 of the original 9,385 samples) may not be ideal for aircraft recognition benchmarking, which was proposed for this very purpose in a [published paper](https://www.sciencedirect.com/science/article/abs/pii/S1568494620300727). The full dataset could serve as a benchmark if the samples weren't all generated from the same few images like this public dataset was. I'll look into this next.

### Convergence
I started my model off with a typical batch size of 32 and a normal learning rate. The model stopped after about 250 epochs, with 30 epochs without improvement. However, the loss hadn't converged just yet. Hence, I experimented with some other settings and observed things that were taught in the theory classes:

* A higher batch size resulted in slower convergence. At the time of writing, it's at epoch 538 and has yet to reach the same loss as the model above, which was trained with a batch size of 32.
* A high learning rate (10x the default) resulted in failure to converge.
* A lower learning rate (0.5x the default) and a smaller batch size (0.5x the default) resulted in quicker convergence.
