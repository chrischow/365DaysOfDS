---
layout: post
title: "Day 29: Python Keras"
subtitle: "Image Classification"
date: 2020-08-23 21:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today's work is a continuation of my CNN model for the **Multi-Type Aircraft Remote Sensing Images (MTARSI)** dataset.

## Major Setback
As I started writing a post on the MTARSI dataset, I realised two big mistakes I made.

### Incorrect Labels
As I was collecting sample images to show the 22 classes, I realised that some aircraft were **labelled wrongly**. It was my failure to check the images for this project, because I wrongly assumed that academics who [employed "specialists"]((https://zenodo.org/record/3464319#.X0IjpcgzaUk)) would ensure that the labels were assigned correctly. The fundamental mistakes were as such:

* Mistaking passenger aircraft (including the 747) for the C-17, C-5, and KC-10
* Confusing the C-17 and the C-5
* Confusing the C-135 with both the C-17 and C-5
* Confusing the P-3 and the C-130
* Classifying all aircraft with tail-mounted engines as the C-21
* Labelled E-2s and E-3s
* Twin-engine propeller aircraft were classified as U-2s
* B-29s were...not B-29s at all - these were actually P-3s
* A-26s were...not A-26s at all
* P-63s were...not P-63s at all

Hence, I did the following:

* Manually re-labelled C-17s, C-5s, KC-10s, C-135s, and P-3s/"B-29s"
* Re-classified C-21s as **Private Jets**
* Re-classified "U-2s" as **Twin Turboprops**
* Combined "A-26s" with "P-63s" in a new category: **Light Aircraft**
* Removed E-2s and B-29s

### Insufficient Diagnostics
I realised that although I was collecting validation accuracy, I wasn't tracking the categories the model was doing well on, and which categories the model wasn't good at picking out. I should have saved the confusion matrices and accuracy scores *per class*.

## Restarting the Testing
Incorporating the changes above and the things I've learned along the way, I've restarted the project, now with 21 classes:

1. A-10
2. B-1
3. B-2
4. B-52
5. Boeing
6. C-5
7. C-17
8. C-130
9. C-135
10. E-3
11. F-15
12. F-16
13. F-18
14. F-22
15. KC-10
16. Light Aircraft
17. P-3
18. Private Jet
19. T-6
20. T-43
21. Twin Turboprop

Each model will be repeated 10 times with the same train/test split. Confusion matrices and accuracy (by class) will be saved.

## New Problem
After trying out the first few runs, I ran into a new problem: the model was stuck at a pretty high loss. In general, this means that the model found a sub-optimal local minimum for loss. Based on some research, the potential problems could be:

1. Data was not labelled correctly
2. Training data was not randomised
3. Learning rate is too high
4. Activation functions are not linear
5. Model complexity is too low
6. Class imbalance not addressed

Somehow, having incorrectly-labelled images helped the model to learn better. One effect of correcting the labels was the exacerbation of class imbalance. The "Boeing" passenger aircraft class now comprised 10.45% of the labels. Coincidentally, the model's validation accuracy was stuck at - you guessed it - **10.45%**.

Therefore, the first course of action was to feed class weights to the model. While these helped the model to move past predicting "Boeing" for all validation samples, the bias was very high, with accuracy at 4-5%.

I decided to check the results of each iteration. After inspecting the confusion matrix for the first iteration of M1, I discovered that the strategy was to predict everything as an E-3, which comprises **4.5%** of the data. In the second iteration, the strategy was to predict everything as a Private Jet. Clearly, something is wrong with the model, and I'll have to work through the potential problems above. I'll do this tomorrow.

## Summary of Key Learning Pointers

### 1. Check Your Data
EDA is extremely important to make sure the model is learning the right things. In my case, I saw good results and took it for granted that the model worked. However, upon realising how dirty the data actually was, I cleaned it, and this resulted in poor results. It shows that the model wasn't built on a strong foundation and might fail to generalise on new data.

### 2. Inspect Your Model
Model transparency is essential for peeking under the hood to see what on earth your model is up to. Yes, results may be good, but it might be for the wrong reasons. In my case, scattering some passenger aircraft images among the C-5s, C-17s, and KC-10s helped to produce good results. However, the model was not learning the right things. Installing a feature to allow me to inspect the confusion matrix of each iteration allowed me to detect the problems with the model.