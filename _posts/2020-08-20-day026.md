---
layout: post
title: "Day 26: Python Keras"
subtitle: "Image Classification"
date: 2020-08-20 23:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today's work is a continuation of my CNN model for the **Multi-Type Aircraft Remote Sensing Images (MTARSI)** dataset.

## Model 4
I trained Model 4, which differed from Model 3 in image size (40x40 instead of 70x70) and CNN architecture. Instead of alternating Convolutional and Max Pooling layers, I created blocks, each comprising two Convolutional layers and one Max Pooling layer.Initially, I set Model 4 up with 2 blocks with 32 filters for each Convolutional layer in the first block, and 64 filters for each Convolutional layer in the second. This did not work out well - the model was not converging after 30-40 epochs. Hence, I switched up the architecture, adding another block.

The new first block had 16 filters in each Convolutional layer. In total, there were 3 blocks, closer to the VGG16 architecture, which contains blocks of 64, 128, and then 512 filters in the Convolutional layers across 5 blocks. Yea, I don't have the computing power for that.

Here's the Model 4 architecture:

```
Model: "Model4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
ConvBlock1_Conv1 (Conv2D)    (None, 48, 48, 16)        448       
_________________________________________________________________
ConvBlock1_Conv2 (Conv2D)    (None, 48, 48, 16)        2320      
_________________________________________________________________
ConvBlock1_MaxPool (MaxPooli (None, 24, 24, 16)        0         
_________________________________________________________________
ConvBlock2_Conv1 (Conv2D)    (None, 24, 24, 32)        4640      
_________________________________________________________________
ConvBlock2_Conv2 (Conv2D)    (None, 24, 24, 32)        9248      
_________________________________________________________________
ConvBlock2_MaxPool (MaxPooli (None, 12, 12, 32)        0         
_________________________________________________________________
ConvBlock3_Conv1 (Conv2D)    (None, 12, 12, 64)        18496     
_________________________________________________________________
ConvBlock3_Conv2 (Conv2D)    (None, 12, 12, 64)        36928     
_________________________________________________________________
ConvBlock3_MaxPool (MaxPooli (None, 6, 6, 64)          0         
_________________________________________________________________
Flatten (Flatten)            (None, 2304)              0         
_________________________________________________________________
Dense1 (Dense)               (None, 32)                73760     
_________________________________________________________________
Dense1_Dropout (Dropout)     (None, 32)                0         
_________________________________________________________________
CLF (Dense)                  (None, 22)                726       
=================================================================
Total params: 146,566
Trainable params: 146,566
Non-trainable params: 0
_________________________________________________________________
```

There was a decrease in performance relative to Model 2. Accuracy was 5 percentage points lower, and it took 28% more epochs. 

<img src="/365DaysOfDS/img/posts/day026-01.png" style='margin-left: auto; margin-right: auto; display: block;'>

## Model 5
Hence, I trained Model 5, which is Model 4 plus another Convolutional layer in the 3rd block. For simplicity, I'll use the notation `Conv3-32`: this means a Convolutional layer with a kernel size of 3 and 32 filters.

Validation accuracy fell even further to 59.55%. It seems that adding more Conv layers in the blocks or reducing the image size has resulted in a **decrease** in performance.

## Revising the Testing Methodology
I explored the various models experimented with by the VGGNet team. I realised that they made incremental changes to models that worked. When I mapped out my approach, I realised that Model 1 to Models 2 and 3 jumped straight into some hyperparameter optimisation before tweaking the architecture - the significant improvement was due to the reduction in image size from 200x200 to 70x70. However, Model 3 to Models 4 and 5 had some pretty big changes: (1) reduction of images size to 48x48 AND (2) a change in the network architecture - cutting off one Conv block.

The problem with this approach is that I can't measure the impact of the changes. Was the drop in performance due to the image size or the architecture? Could it be both? Hence, I'm rolling back to Model 2 and starting from there.

I'm also writing a script to run models back-to-back overnight. That way I can test out more configurations without affecting evening computer use.