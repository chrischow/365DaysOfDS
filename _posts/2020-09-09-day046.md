---
layout: post
title: "Day 46: Data Science Readings"
subtitle: "Machine Learning"
date: 2020-09-08 22:00:00 +0800
background: '/img/posts/ml.jpg'
---

Today is the first day I'm taking a break from coding. I'll be doing a summarising a book I'm reading: [Machine Learning Yearning by Andrew Ng](https://www.goodreads.com/book/show/30741739-machine-learning-yearning). I probably will not complete the summary today. I'll add a cheatsheet with the full summary when I'm done.

## Machine Learning and Scale
* Traditional learning algorithms may or may not do better on small datasets, but certainly don't do as well on large datasets as neural networks (NNs)
* Typically, with more data, the bigger the NN, the better the performance
* The reliable ways to improve an algorithm's performance are usually:
    * Train a bigger network
    * Get more data

## Setting Up Dev and Test Sets

### Definitions
* Training set: Set to run the learning algorithm on
* Dev set (aka hold-out cross validation set): Used to tune parameters, select features, and make other decisions regarding the learning algorithm
* Test set: Used to evaluate the performance of the algorithm, but not to make any decisions regarding what learning algorithm or parameters to use

### Choosing Datasets
* Choose dev and test sets to reflect data you expect to get in the future and want to do well on
* Don't assume your training distribution is the same as your test distribution
* If dev and test sets come from the **same distribution** and the test set result was bad, it means there was overfitting of the dev set
* If dev and test sets come from different distributions, these could have gone wrong:
    * You overfit to the dev set
    * The test set is harder than the dev set (algorithm is ok)
    * The test set is just different from the dev set

### Dataset Sizes
* Key principle: Should be big enough to allow you to detect difference between algorithms that you are comparing
* Dev set:
    * Typical sizes: 1,000 to 10,000
* Test set:
    * For small datasets e.g. 100 to 10,000 samples: 30% of data
    * For big datasets: look at the absolute number and make sure there's enough 

### Establish a Single-Number Evaluation Metric to Optimise
* Allows you to sort models according to their performance on this metric
* If there are multiple related metrics, find a way to combine them e.g. F1 score, weighted average
* Take at most a week to do this + get a test/dev set ready

### Satisficing Metrics
* If there are metrics that can't be combined, define an acceptable level ("good enough") for one metric, and optimise the other metric
* For N total metrics, set N-1 as satisficing metrics

### Iterative Modelling
1. Start off with some **idea**
2. Implement the idea in **code**
3. Carry out an **experiment** - based on what you learned, generate more ideas and go back to step 1

### When to Change Dev/Test Sets and Metrics
1. Actual distribution is different from the dev/test sets: make dev/tests more representative
2. You have overfit to the dev set: get a fresh one
3. The metric is measuring something other than what the project needs to optimise: Change the metric (e.g. adjust penalisation weights)

### Summary from Book
* Choose dev and test sets from a distribution that reflects what data you expect to get in the future and want to do well on. This may not be the same as your training data’s distribution.
* Choose dev and test sets from the same distribution if possible.
* Choose a single-number evaluation metric for your team to optimize. If there are multiple goals that you care about, consider combining them into a single formula (such as averaging multiple error metrics) or defining satisficing and optimizing metrics.
* Machine learning is a highly iterative process: You may try many dozens of ideas before finding one that you’re satisfied with.
* Having dev/test sets and a single-number evaluation metric helps you quickly evaluate algorithms, and therefore iterate faster.
* When starting out on a brand new application, try to establish dev/test sets and a metric quickly, say in less than a week. It might be okay to take longer on mature applications.
* The old heuristic of a 70%/30% train/test split does not apply for problems where you have a lot of data; the dev and test sets can be much less than 30% of the data.
* Your dev set should be large enough to detect meaningful changes in the accuracy of your algorithm, but not necessarily much larger. Your test set should be big enough to give you a confident estimate of the final performance of your system.
* If your dev set and metric are no longer pointing your team in the right direction, quickly change them: (i) If you had overfit the dev set, get more dev set data. (ii) If the actual distribution you care about is different from the dev/test set distribution, get new dev/test set data. (iii) If your metric is no longer measuring what is most important to you, change the metric.

## Basic Error Analysis

### Build Your First System Quickly, Then Iterate
* Build a system quickly and examine how it functions
* This will give clues to show the most promising directions to invest your time

### Error Analysis: Look at Dev Set Examples to Evaluate Ideas
* Error analysis: the process of examining dev set examples that your algorithm misclassified, so that you can understand the underlying causes of the errors
* Estimate the maximum possible improvement from a potential idea (e.g. implementing something to predict a specific class correctly will lower the error by only 5 percentage points)
* Objective is to eliminate categories of errors

### Evaluate Multiple Ideas in Parallel
* Start with enumerating potential ideas in columns
* Put misclassified examples in the rows and see which of them can be corrected using the potential ideas
* Evaluate all these ideas at one shot so you only pass over the set of misclassified examples once
* Add ideas in new columns if you generate them
* Record comments on the possible reasons why the examples were misclassified

### Cleaning Up Mislabelled Dev and Test Set Examples
* In error analysis, also keep track of mislabelled examples
* If the proportion of misclassified examples due to mislabelling was high, it might be worth fixing them
* Otherwise, it's not crucial
* Check the labelling of examples that were labelled correctly as well

### Split Large Dev Set in Two
If dev set is sufficiently large, split it in two:

* Reason is that manual error analysis could lead to overfitting on the misclassified examples inspected
* Blackbox dev set:
    * For tuning parameters
    * Should be about 1,000-10,000 examples
    * If the overall dev set is too small, might have to use the whole dev set as the Eyeball dev set
* Eyeball dev set:
    * For error analysis
    * Will likely overfit to this faster
    * For a problem that even humans don't do well, the Eyeball dev set may not be as helpful and you can omit it
    * Choose size of Eyeball dev set based on the number of mistakes made on it:
        * If error rate is 5%, then an Eyeball set of about 2,000 is required to have 100 misclassified examples
    * With ample data, choose a size based on how much time you have to manually analyse the data
* Guidelines for choosing size of Eyeball set (divide by error rate)
    * ~10 mistakes: Too small, but if the dataset itself it's small, just make do with it
    * ~20 mistakes: Should be able to get a sense of the major error sources
    * ~50 mistakes: Should get a good sense
    * ~100 mistakes: Very good sense
* Eyeball dev set is more important and the overall dev set should be dedicated to this
    * Can use for error analyses, model selection, and hyperparameter tuning
    * Downside is that there is a risk of overfitting

### Book Summary
* When you start a new project, especially if it is in an area in which you are not an expert, it is hard to correctly guess the most promising directions.
* So don’t start off trying to design and build the perfect system. Instead build and train a basic system as quickly as possible—perhaps in a few days. Then use error analysis to help you identify the most promising directions and iteratively improve your algorithm from there.
* Carry out error analysis by manually examining ~100 dev set examples the algorithm misclassifies and counting the major categories of errors. Use this information to prioritize what types of errors to work on fixing.
* Consider splitting the dev set into an Eyeball dev set, which you will manually examine, and a Blackbox dev set, which you will not manually examine. If performance on the Eyeball dev set is much better than the Blackbox dev set, you have overfit the Eyeball dev set and should consider acquiring more data for it.
* The Eyeball dev set should be big enough so that your algorithm misclassifies enough examples for you to analyze. A Blackbox dev set of 1,000-10,000 examples is sufficient for many applications.
* If your dev set is not big enough to split this way, just use the entire dev set as an Eyeball dev set for manual error analysis, model selection, and hyperparameter tuning.